{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.5.0"
      ],
      "metadata": {
        "id": "Ugyb75zOXnGN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "outputId": "125c13d8-6aa4-4e13-c3e1-ee7fe22d4cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.5.0\n",
            "  Downloading matplotlib-3.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting setuptools-scm>=4\n",
            "  Downloading setuptools_scm-7.0.5-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.0) (1.21.6)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.37.4-py3-none-any.whl (960 kB)\n",
            "\u001b[K     |████████████████████████████████| 960 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.0) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.0) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.0) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.0) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.5.0) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib==3.5.0) (57.4.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib==3.5.0) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from setuptools-scm>=4->matplotlib==3.5.0) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->setuptools-scm>=4->matplotlib==3.5.0) (3.9.0)\n",
            "Installing collected packages: setuptools-scm, fonttools, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed fonttools-4.37.4 matplotlib-3.5.0 setuptools-scm-7.0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import *\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "plt.rcParams['figure.figsize'] = [6.0, 6.0]\n",
        "class Arrow3D(FancyArrowPatch):\n",
        "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
        "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
        "        self._verts3d = xs, ys, zs\n",
        "    \n",
        "    def do_3d_projection(self, renderer=None):\n",
        "        xs3d, ys3d, zs3d = self._verts3d\n",
        "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, self.axes.M)\n",
        "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
        "\n",
        "        return np.min(zs)\n",
        "\n",
        "    def draw(self, renderer):\n",
        "        xs3d, ys3d, zs3d = self._verts3d\n",
        "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
        "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]),)\n",
        "        FancyArrowPatch.draw(self, renderer)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "ZTnmixTVC0-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METHODS\n",
        "beta = 0.4\n",
        "h = 1e-2\n",
        "alpha = 1e-4\n",
        "decay_steps=  10\n",
        "import torch\n",
        "import math\n",
        "import decimal\n",
        "import copy\n",
        "class Ours:\n",
        "  def __init__(self, params, steps, h, loss_func,plr,beta=0.9, eps=1e-3, term_eps=1e-3):\n",
        "    self.params = params\n",
        "    self.radius = 0\n",
        "    self.eps = eps\n",
        "    self.p1 = None\n",
        "    self.p2 = None\n",
        "    self.p1_grad = None\n",
        "    self.p2_grad = None\n",
        "    self.steps = steps\n",
        "    self.h = h\n",
        "    self.loss_func = loss_func\n",
        "    self.plr = plr\n",
        "    self.iter = 0\n",
        "    self.avg_theta = 0\n",
        "    self.h_backup = h\n",
        "    self.p_moment = 0\n",
        "    self.beta = beta\n",
        "    self.avg_depth = 0\n",
        "    self.term_eps = 0\n",
        "    self.iter = 0\n",
        "    self.flag = 1\n",
        "    self.avg_sum=0\n",
        "    self.avg_sum = 0\n",
        "    self.avg_radius = torch.tensor([0], device='cpu')\n",
        "    self.plane_radius = torch.tensor([0], device='cpu')\n",
        "    self.set_locality_flag = 1\n",
        "\n",
        "\n",
        "  def make_rand_vector(self, dims):\n",
        "      vec = torch.rand(dims, requires_grad=True)\n",
        "\n",
        "      return vec\n",
        "\n",
        "  def angle(self, a, b):\n",
        "      a = a.detach()\n",
        "      b = b.detach()\n",
        "      a = a/torch.norm(a)\n",
        "      b = b/torch.norm(b)\n",
        "\n",
        "      term = torch.tensor([float(decimal.Decimal(torch.norm(a-b).item())/decimal.Decimal(torch.norm(a+b).item()))], device='cpu')\n",
        "\n",
        "      angle = 2 * torch.atan(term)\n",
        "      return angle  #* 180/torch.pi\n",
        "\n",
        "  def projection(self,p):\n",
        "    v = p - self.plane_point\n",
        "    projected_normal = torch.dot(v, self.plane_normal)\n",
        "    projected_point = p - projected_normal\n",
        "    return projected_point\n",
        "\n",
        "\n",
        "  def step(self, loss):\n",
        "    self.iter += 1\n",
        "    flag = 0\n",
        "\n",
        "    self.p1 = copy.deepcopy(self.params)\n",
        "    grad_x = torch.autograd.grad(loss, self.params, create_graph=True, retain_graph=True )\n",
        "    grad_x_vec = torch.cat([g.contiguous().view(-1) for g in grad_x])#.reshape(-1,1)\n",
        "    grad_x_vec_d = grad_x_vec.detach()\n",
        "    norm_grad = torch.norm(grad_x_vec, 2)\n",
        "    if norm_grad <= self.term_eps:\n",
        "      return\n",
        "\n",
        "    self.p1_grad = grad_x_vec\n",
        "    self.p1_test = 0\n",
        "\n",
        "    ## GOING PERPENDICULAR\n",
        "\n",
        "    rand_vec = self.make_rand_vector(self.p1_grad.shape).to('cpu')\n",
        "    perp_vec = rand_vec - ((rand_vec.T @ self.p1_grad)/(torch.pow(torch.norm(self.p1_grad,2),2))) * self.p1_grad\n",
        "    self.rand_vec = rand_vec\n",
        "\n",
        "    norm_len = torch.norm(perp_vec, 2)\n",
        "\n",
        "\n",
        "    perp_vec = (perp_vec / (torch.norm(perp_vec, 2)))\n",
        "\n",
        "    \n",
        "    for (opt_param, p1_param) in zip(self.params, self.p1):\n",
        "        opt_param.data.fill_(p1_param.data)\n",
        "    \n",
        "\n",
        "\n",
        "    step_length = 0\n",
        "\n",
        "\n",
        "\n",
        "    self.perp_vec = perp_vec\n",
        "    self.grad_normal = grad_x_vec\n",
        "\n",
        "\n",
        "\n",
        "    step_length += torch.norm(perp_vec.detach(), 2) * self.h #/ (torch.sqrt(self.p_moment) + 1e-7)\n",
        "    index = 0\n",
        "    for p in self.params:\n",
        "        term = perp_vec[index: index + p.numel()].detach().reshape(p.shape)\n",
        "\n",
        "        p.data.add_((-1) * (term * self.h))\n",
        "        index += p.numel()\n",
        "\n",
        "    loss_val = self.loss_func()\n",
        "\n",
        "    grad_x = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )\n",
        "    grad_x_vec = torch.cat([g.contiguous().view(-1) for g in grad_x])#.reshape(-1,1)\n",
        "    grad_x_vec_d = grad_x_vec.detach()\n",
        "    self.p2_grad = grad_x_vec.clone()\n",
        "    ang_g1g2 = math.ceil(self.angle(self.p1_grad, self.p2_grad)* 180/torch.pi)\n",
        "    ang_p1g2 = math.ceil(self.angle(self.perp_vec, self.p2_grad) * 180/torch.pi)\n",
        "\n",
        "    if(ang_p1g2 < 90):\n",
        "        self.p_moment = 0\n",
        "        #self.avg_locality =0\n",
        "        #self.iter = 0\n",
        "        self.flag = 1\n",
        "\n",
        "    #if(ang_g1g2 <= 10 and ang_p1g2 > 90 ): #\n",
        "    #    #print(\"OKAY\")\n",
        "    #    self.h = self.h_backup\n",
        "    #    flag = 1\n",
        "    #else:\n",
        "        #print(\"G1_G2\", ang_g1g2, \"P1 G2\", ang_p1g2, \" H \", self.h, \"DISTANCE \", torch.norm(self.p1_grad - self.p2_grad, 2).item())\n",
        "    #    self.h = self.h / 10\n",
        "\n",
        "\n",
        "    #    #print(torch.norm(p3_vec))\n",
        "    #    #print(\" TRAPPED \", torch.norm(param_vec))\n",
        "    #    flag = 0\n",
        "\n",
        "    # DONE PERPENDICULAR\n",
        "\n",
        "    #print(self.p1)\n",
        "    self.p2 = copy.deepcopy(self.params)\n",
        "    p1_vec = torch.cat([g.contiguous().view(-1) for g in self.p1])\n",
        "\n",
        "    self.theta = self.angle(self.p1_grad, self.p2_grad) + self.eps\n",
        "    #print(\"THETA \" , self.theta.item() * 180/torch.pi)\n",
        "    import time\n",
        "    #time.sleep(0.5)\n",
        "\n",
        "    #step_length = self.h * self.steps\n",
        "    self.radius = step_length * (torch.cos(self.theta)/torch.sin(self.theta))\n",
        "\n",
        "    index = 0\n",
        "    #self.params = copy.deepcopy(self.p1)\n",
        "    for param,p1_param in zip(self.params, self.p1):\n",
        "        param.data.fill_(p1_param.data)\n",
        "\n",
        "    if(self.radius < self.avg_radius):\n",
        "        #print(\"\\nJUMPING \",self.flag, \"\\n\")\n",
        "        self.radius = 2.0 * self.radius\n",
        "    self.normalized_grad = (self.p1_grad/(torch.norm(self.p1_grad,2))) * self.radius\n",
        "\n",
        "    for p in self.params:\n",
        "\n",
        "        term = self.normalized_grad[index: index + p.numel()].detach().reshape(p.shape)\n",
        "        #print(\"TERM \", term)\n",
        "        p.data.add_((-1)*term)\n",
        "        index += p.numel()\n",
        "    p2_vec = torch.cat([g.contiguous().view(-1) for g in self.p2])\n",
        "    #print(\" DISTANCE     \", torch.norm(p1_vec-p2_vec))\n",
        "    self.avg_theta += self.theta * 180/torch.pi\n",
        "    self.avg_theta = self.avg_theta/self.iter\n",
        "    self.avg_radius = beta * self.avg_radius + (1-beta) * self.radius\n",
        "    \n"
      ],
      "metadata": {
        "id": "Q6GRn-_G3Max"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AngularGradCos(object):\n",
        "    def __init__(self, params,lrn_rate, beta1, beta2, eps):\n",
        "        self.x = params\n",
        "        self.params = params\n",
        "        self.lrn_rate = lrn_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.idx = 0\n",
        "        self.m = None # 1st order\n",
        "        self.v = None # 2nd order\n",
        "        self.g_prev = 0.0\n",
        "        self.min = 0.0\n",
        "\n",
        "    def step(self, loss_val):\n",
        "        self.idx += 1\n",
        "        g = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )[0].detach()\n",
        "        self.g = g\n",
        "        if self.m == None or self.v == None:\n",
        "          self.m = torch.zeros_like(g)\n",
        "          self.v = torch.zeros_like(g)\n",
        "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * g\n",
        "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * g ** 2\n",
        "        m_adj = self.m / (1.0 - np.power(self.beta1, self.idx))\n",
        "        v_adj = self.v / (1.0 - np.power(self.beta2, self.idx))\n",
        "\n",
        "        \n",
        "        tan_theta = abs((self.g_prev - g) / (1 + self.g_prev * g))\n",
        "        cos_theta = 1 / torch.sqrt(1 + torch.square(tan_theta))\n",
        "        angle = np.arctan(tan_theta) * (180 / 3.141592653589793238)\n",
        "      \n",
        "        \n",
        "        self.min = angle\n",
        "        diff = abs(self.g_prev - g)\n",
        "        final_cos_theta = cos_theta\n",
        "\n",
        "\n",
        "        dfc = 1.0 / (1.0 + np.exp(final_cos_theta))\n",
        "        term_ex =  self.lrn_rate * m_adj * dfc / (np.sqrt(v_adj) + self.eps)\n",
        "        index = 0\n",
        "        for p in self.params:\n",
        "\n",
        "          term = term_ex[index: index + p.numel()].detach().reshape(p.shape)\n",
        "          #print(\"TERM \", term)\n",
        "          p.data.add_((-1)*term)\n",
        "          self.g_prev = g\n",
        "          index += p.numel()\n",
        "        \n",
        "class Adam(object):\n",
        "    def __init__(self, params,  lrn_rate, beta1, beta2, eps):\n",
        "        self.lrn_rate = lrn_rate\n",
        "        self.params = params\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.idx = 0\n",
        "        self.m = None # 1st order\n",
        "        self.v = None # 2nd order\n",
        "\n",
        "    def step(self, loss_val):\n",
        "        self.idx += 1\n",
        "        g = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )[0].detach()\n",
        "        self.g = g\n",
        "        if self.m == None or self.v == None:\n",
        "          self.m = torch.zeros_like(g)\n",
        "          self.v = torch.zeros_like(g)\n",
        "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * g\n",
        "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * g ** 2\n",
        "        m_adj = self.m / (1.0 - np.power(self.beta1, self.idx))\n",
        "        v_adj = self.v / (1.0 - np.power(self.beta2, self.idx))\n",
        "        term_ex = self.lrn_rate * m_adj / np.sqrt(v_adj + self.eps)\n",
        "        index = 0\n",
        "        for p in self.params:\n",
        "\n",
        "          term = term_ex[index: index + p.numel()].detach().reshape(p.shape)\n",
        "          #print(\"TERM \", term)\n",
        "          p.data.add_((-1)*term)\n",
        "          self.g_prev = g\n",
        "          index += p.numel()\n",
        "\n",
        "class RMSprop(object):\n",
        "    def __init__(self, params,  lrn_rate, beta1, beta2, eps):\n",
        "        self.lrn_rate = lrn_rate\n",
        "        self.params = params\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.idx = 0\n",
        "        self.m = None # 1st order\n",
        "        self.v = None # 2nd order\n",
        "\n",
        "    def step(self, loss_val):\n",
        "        self.idx += 1\n",
        "        g = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )[0].detach()\n",
        "        self.g = g\n",
        "        if self.m == None or self.v == None:\n",
        "          self.m = torch.zeros_like(g)\n",
        "          self.v = torch.zeros_like(g)\n",
        "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * g\n",
        "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * g ** 2\n",
        "        m_adj = self.m / (1.0 - np.power(self.beta1, self.idx))\n",
        "        v_adj = self.v / (1.0 - np.power(self.beta2, self.idx))\n",
        "        term_ex = self.lrn_rate * g / np.sqrt(v_adj + self.eps)\n",
        "        index = 0\n",
        "        for p in self.params:\n",
        "\n",
        "          term = term_ex[index: index + p.numel()].detach().reshape(p.shape)\n",
        "          #print(\"TERM \", term)\n",
        "          p.data.add_((-1)*term)\n",
        "          self.g_prev = g\n",
        "          index += p.numel()\n",
        "\n",
        "class AdaBelief(object):\n",
        "    def __init__(self, params,  lrn_rate, beta1, beta2, eps):\n",
        "        self.lrn_rate = lrn_rate\n",
        "        self.params = params\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.idx = 0\n",
        "        self.m = None # 1st order\n",
        "        self.v = None # 2nd order\n",
        "\n",
        "    def step(self, loss_val):\n",
        "        self.idx += 1\n",
        "        g = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )[0].detach()\n",
        "        self.g = g\n",
        "        if self.m == None or self.v == None:\n",
        "          self.m = torch.zeros_like(g)\n",
        "          self.v = torch.zeros_like(g)\n",
        "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * g\n",
        "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * (g - self.m) ** 2 + self.eps\n",
        "        m_adj = self.m / (1.0 - np.power(self.beta1, self.idx))\n",
        "        v_adj = self.v / (1.0 - np.power(self.beta2, self.idx))\n",
        "        term_ex = self.lrn_rate * m_adj / np.sqrt(v_adj + self.eps)\n",
        "        index = 0\n",
        "        for p in self.params:\n",
        "\n",
        "          term = term_ex[index: index + p.numel()].detach().reshape(p.shape)\n",
        "          #print(\"TERM \", term)\n",
        "          p.data.add_((-1)*term)\n",
        "          self.g_prev = g\n",
        "          index += p.numel()\n",
        "\n",
        "class diffGrad(object):\n",
        "    def __init__(self, params, lrn_rate, beta1, beta2, eps):\n",
        "        self.lrn_rate = lrn_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "        self.idx = 0\n",
        "        self.m = None # 1st order\n",
        "        self.v = None # 2nd order\n",
        "        self.g_prev = 0.0\n",
        "\n",
        "    def step(self, loss_val):\n",
        "        self.idx += 1\n",
        "        g = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )[0].detach()\n",
        "        self.g = g\n",
        "        if self.m == None or self.v == None:\n",
        "          self.m = torch.zeros_like(g)\n",
        "          self.v = torch.zeros_like(g)\n",
        "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * g\n",
        "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * g ** 2\n",
        "        m_adj = self.m / (1.0 - np.power(self.beta1, self.idx))\n",
        "        v_adj = self.v / (1.0 - np.power(self.beta2, self.idx))\n",
        "        dfc = 1.0 / (1.0 + np.exp(-np.abs(self.g_prev - g)))\n",
        "        term_ex = self.lrn_rate * m_adj * dfc / (np.sqrt(v_adj) + self.eps)\n",
        "        index = 0\n",
        "        for p in self.params:\n",
        "\n",
        "          term = term_ex[index: index + p.numel()].detach().reshape(p.shape)\n",
        "          #print(\"TERM \", term)\n",
        "          p.data.add_((-1)*term)\n",
        "          self.g_prev = g\n",
        "          index += p.numel()\n",
        "        self.g_prev = g\n",
        "        \n",
        "class AngularGradTan(object):\n",
        "    def __init__(self, params, lrn_rate, beta1, beta2, eps):\n",
        "        self.lrn_rate = lrn_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "        self.idx = 0\n",
        "        self.m = None # 1st order\n",
        "        self.v = None # 2nd order\n",
        "        self.g_prev = 0.0\n",
        "        self.min = 0.0\n",
        "\n",
        "    def step(self, loss_val):\n",
        "        self.idx += 1\n",
        "        g = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )[0].detach()\n",
        "        self.g = g\n",
        "        if self.m == None or self.v == None:\n",
        "          self.m = torch.zeros_like(g)\n",
        "          self.v = torch.zeros_like(g)\n",
        "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * g\n",
        "        self.v = self.beta2 * self.v + (1.0 - self.beta2) * g ** 2\n",
        "        m_adj = self.m / (1.0 - np.power(self.beta1, self.idx))\n",
        "        v_adj = self.v / (1.0 - np.power(self.beta2, self.idx))\n",
        "\n",
        "        tan_theta = abs((self.g_prev - g) / (1 + self.g_prev * g))\n",
        "        cos_theta = 1 / np.sqrt(1 + tan_theta**2)\n",
        "        angle = np.arctan(tan_theta) * (180 / 3.141592653589793238)\n",
        "     \n",
        "        \n",
        "        self.min = angle\n",
        "        diff = abs(self.g_prev - g)\n",
        "        final_tan_theta = tan_theta\n",
        "\n",
        "        dfc = 1.0 / (1.0 + np.exp(final_tan_theta))\n",
        "        term_ex = self.lrn_rate * m_adj * dfc / (np.sqrt(v_adj) + self.eps)\n",
        "        index = 0\n",
        "        for p in self.params:\n",
        "\n",
        "          term = term_ex[index: index + p.numel()].detach().reshape(p.shape)\n",
        "          #print(\"TERM \", term)\n",
        "          p.data.add_((-1)*term)\n",
        "          self.g_prev = g\n",
        "          index += p.numel()\n",
        "        self.g_prev = g\n",
        "        \n",
        "\n",
        "\n",
        "class SGDM(object):\n",
        "    def __init__(self, params, lrn_rate, beta1, eps):\n",
        "        self.lrn_rate = lrn_rate\n",
        "        self.beta1 = beta1\n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "        self.idx = 0\n",
        "        self.m = None # 1st order\n",
        "        self.v = None # 2nd order\n",
        "        \n",
        "    def step(self, loss_val):\n",
        "        self.idx += 1\n",
        "        g = torch.autograd.grad(loss_val, self.params, create_graph=True,retain_graph=True )[0].detach()\n",
        "        self.g = g\n",
        "        if self.m == None or self.v == None:\n",
        "          self.m = torch.zeros_like(g)\n",
        "          self.v = torch.zeros_like(g)\n",
        "        self.m = self.beta1 * self.m + (1.0 - self.beta1) * g\n",
        "        \n",
        "        m_adj = self.m / (1.0 - np.power(self.beta1, self.idx))\n",
        "        term_ex =  self.lrn_rate * m_adj\n",
        "        index = 0\n",
        "        for p in self.params:\n",
        "\n",
        "          term = term_ex[index: index + p.numel()].detach().reshape(p.shape)\n",
        "          #print(\"TERM \", term)\n",
        "          p.data.add_((-1)*term)\n",
        "          self.g_prev = g\n",
        "          index += p.numel()"
      ],
      "metadata": {
        "id": "N1SZHRdPleb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function definition"
      ],
      "metadata": {
        "id": "Yz5kLknHLTSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def func_1():\n",
        "  global var\n",
        "  x = var\n",
        "  x1 = x[0]\n",
        "  x2 = x[0]\n",
        "\n",
        "  return -torch.sin(x1) * torch.pow(x2,2)\n",
        "\n",
        "def func_1_graph(x):\n",
        "  x1 = x[0]\n",
        "  x2 = x[0]\n",
        "\n",
        "  return -np.sin(x1) * pow(x2,2)\n",
        "\n",
        "def func_2_graph(x):\n",
        "  x1 = x[0]\n",
        "  x2 = x[1]\n",
        "  return -np.sin((pow(x1,2) + pow(x2,2)))/( pow(x1,2) + pow(x2,2))\n",
        "\n",
        "\n",
        "def func_2():\n",
        "  global var\n",
        "  x = var\n",
        "  x1 = x[0]\n",
        "  x2 = x[1]\n",
        "\n",
        "  return -torch.sin((pow(x1,2) + pow(x2,2)))/( pow(x1,2) + pow(x2,2))\n",
        "\n",
        "\n",
        "def func_3_graph(x):\n",
        "\n",
        "  x1 = x[0]\n",
        "  x2 = x[1]\n",
        "\n",
        "  q1 = torch.full(x1.shape, -0.65)\n",
        "  q2 = torch.full(x1.shape, -0.35)\n",
        "  q3 = torch.full(x1.shape, -0.05)\n",
        "  q4 = torch.full(x1.shape, 0.25)\n",
        "  q5 = torch.full(x1.shape, 0.55)\n",
        "\n",
        "  t1 = torch.add(torch.sign(q1-x1), torch.sign(q2-x1))\n",
        "  t2 = torch.add(t1, torch.sign(q3-x1))\n",
        "  t3 = torch.add(t1, t2)\n",
        "  t4 = torch.add(torch.sign(q4-x1), torch.sign(q5-x1))\n",
        "  t5 = torch.add(t3,t4)\n",
        "  \n",
        "\n",
        "\n",
        "  return ((t5.numpy())/7+np.sin(5*x1)*np.cos(5*x2)/100)\n",
        "\n",
        "def func_3():\n",
        "  global var\n",
        "  x = var\n",
        "  x1 = x[0]\n",
        "  x2 = x[1]\n",
        "\n",
        "  q1 = torch.full_like(x1, -0.65)\n",
        "  q2 = torch.full_like(x1, -0.35)\n",
        "  q3 = torch.full_like(x1, -0.05)\n",
        "  q4 = torch.full_like(x1, 0.25)\n",
        "  q5 = torch.full_like(x1, 0.55)\n",
        "\n",
        "  t1 = torch.add(torch.sign(q1-x1), torch.sign(q2-x1))\n",
        "  t2 = torch.add(t1, torch.sign(q3-x1))\n",
        "  t3 = torch.add(t1, t2)\n",
        "  t4 = torch.add(torch.sign(q4-x1), torch.sign(q5-x1))\n",
        "  t5 = torch.add(t3,t4)\n",
        "  \n",
        "\n",
        "\n",
        "  return ((t5)/7 + torch.sin(5*x1)*torch.cos(5*x2)/100)\n"
      ],
      "metadata": {
        "id": "xK4GPzT9LHoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function 1 train and plot"
      ],
      "metadata": {
        "id": "TiYsiLk1LH1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOOP \n",
        "\n",
        "\n",
        "# TRAINING LOOP\n",
        "epochs = 1000\n",
        "func = func_1##func_3\n",
        "func_graph = func_1_graph##func_3_graph\n",
        "\n",
        "lrn_rate = 1e-2\n",
        "beta1 = 0.95\n",
        "beta2 = 0.999\n",
        "eps = 1e-2\n",
        "eps_t = 1e-5\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "  var = torch.tensor([-2.0,0.0], requires_grad=True)\n",
        "  optimizers = {\n",
        "    \"Ours\" :  Ours(params=var, steps=1, h=lrn_rate, loss_func= func,plr=h,beta=0.9,eps = eps,term_eps=eps_t),\n",
        "    \"AngularGradCos\" :  AngularGradCos(params = var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"AngularGradTan\" : AngularGradTan(params = var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"Adam\" : Adam(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"RMSprop\" : RMSprop(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"AdaBelief\" : AdaBelief(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"diffGrad\" : diffGrad(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"SGDM\" : SGDM(params=var, lrn_rate=lrn_rate, beta1=beta1, eps=eps),\n",
        "    \n",
        "\n",
        "  }\n",
        "  method_name = list(optimizers.keys())[i]\n",
        "  optimizer = optimizers[method_name]\n",
        "  print(method_name)\n",
        "  points = []\n",
        "  x_coord, y_coord = optimizer.params\n",
        "  z_coord = func_graph(optimizer.params.detach())\n",
        "  x_coord = x_coord.item()\n",
        "  y_coord = y_coord.item()\n",
        "  z_coord = z_coord.item()\n",
        "  points.append([x_coord, y_coord, z_coord])\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    #print(epoch)\n",
        "    loss_value = func()\n",
        "    optimizer.step(loss_value)\n",
        "    x_coord, y_coord = optimizer.params\n",
        "    z_coord = func_graph(optimizer.params.detach())\n",
        "    x_coord = x_coord.item()\n",
        "    y_coord = y_coord.item()\n",
        "    z_coord = z_coord.item()\n",
        "    points.append([x_coord, y_coord, z_coord])\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "  points = np.array(points)\n",
        "  #points = np.array([[-2,2,func_1([-2,2])], [-2,0, func_1([-2,0])],])# [-2,1, func_1([-2,1])], [0,0, func_1([0,0])]])\n",
        "  fig = plt.figure()\n",
        "  axes = fig.gca(projection ='3d', computed_zorder=False)\n",
        "  #print(\"OPTIMAL \", optimizer.params)\n",
        "\n",
        "  x = np.arange(-3,3,0.1)\n",
        "  y = np.arange(-3,3,0.1)\n",
        "  axis_x,axis_y = np.meshgrid(x,y)\n",
        "\n",
        "\n",
        "\n",
        "  z = func_graph([axis_x, axis_y])\n",
        "  #print(axis_x)\n",
        "\n",
        "  surf = axes.plot_surface(axis_x, axis_y, z, alpha=0.6, zorder=0,)\n",
        "  surf._facecolors2d = surf._facecolor3d\n",
        "  surf._edgecolors2d = surf._edgecolor3d\n",
        "  #axes.plot3D(points[:,0], points[:,1], points[:,2],'red')\n",
        "\n",
        "  for i,v in enumerate(points[:len(points)-1]):\n",
        "      #ax.plot([mean_x,v[0]], [mean_y,v[1]], [mean_z,v[2]], color='red', alpha=0.8, lw=3)\n",
        "      #I will replace this line with:\n",
        "      a = Arrow3D([points[i,0], points[i+1, 0]], [points[i,1], points[i+1, 1]], \n",
        "                  [points[i,2], points[i+1, 2]], mutation_scale=10, \n",
        "                  lw=2, arrowstyle=\"-|>\", color=\"red\")\n",
        "      a.set_zorder(1)\n",
        "      axes.add_artist(a)\n",
        "\n",
        "  axes.set_xlabel(\"x\")\n",
        "  axes.set_ylabel(\"y\")\n",
        "  axes.set_zlabel(\"function value\", rotation=45)\n",
        "  axes.set_title(method_name,fontsize=20)\n",
        "\n",
        "  for point in points[1:len(points)-1]:\n",
        "    axes.scatter3D(points[:,0], points[:,1], points[:,2], c='red', zorder = 3)\n",
        "  axes.scatter3D(-2.0,0.0, func_graph(torch.tensor([-2.0,0.0])), s = 140, c='orange' ,marker='*' , zorder = 3,label='start')\n",
        "  axes.scatter3D(points[-1,0], points[-1,1], points[-1,2], s=140, c='black' ,marker='*' , zorder = 3, label= 'end')\n",
        "  axes.legend(loc=\"upper right\", bbox_to_anchor=(1.15,0.8))\n",
        "  axes.view_init(10, 60)\n",
        "  #axes.view_init(40, 60)\n",
        "  fig.savefig(method_name+\".png\", dpi=300, format='png', bbox_inches='tight')\n",
        "  fig.clf()\n",
        "  plt.clf()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "HuyS0K2ALIzh",
        "outputId": "a6b55258-b2b2-4e2f-8280-ff144f811f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ours\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:66: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: MatplotlibDeprecationWarning: \n",
            "The M attribute was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use self.axes.M instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AngularGradCos\n",
            "AngularGradTan\n",
            "Adam\n",
            "RMSprop\n",
            "AdaBelief\n",
            "diffGrad\n",
            "SGDM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L_MBrkkxT_yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function 2 train and plot"
      ],
      "metadata": {
        "id": "UrYeAbubLJr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOOP \n",
        "\n",
        "\n",
        "# TRAINING LOOP\n",
        "epochs = 1000\n",
        "func = func_2##func_3\n",
        "func_graph = func_2_graph##func_3_graph\n",
        "#optimizer =  new_gd_triangle_dl_refined_depth_skipping(params=var, steps=1, h=h, loss_func= func,plr=h,beta=0.9)\n",
        "\n",
        "\n",
        "lrn_rate = 1e-2\n",
        "beta1 = 0.95\n",
        "beta2 = 0.999\n",
        "eps = 1e-3#0.00000001\n",
        "eps_t = 1e-5\n",
        "start_point = [3.0,3.0]\n",
        "#optimizer = AngularGradCos(params = var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps)\n",
        "#optimizer = AngularGradTan(params = var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps)\n",
        "#optimizer = Adam(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps)\n",
        "#optimizer = AdaBelief(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps)\n",
        "#optimizer = diffGrad(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps)\n",
        "#optimizer = SGDM(params=var, lrn_rate=lrn_rate, beta1=beta1, eps=eps)\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "  var = torch.tensor(start_point, requires_grad=True)\n",
        "  optimizers = {\n",
        "    \"Ours\" :  Ours(params=var, steps=1, h=lrn_rate, loss_func= func,plr=h,beta=0.9, eps=eps,term_eps=eps_t),\n",
        "    \"Ours\" :  Ours(params=var, steps=1, h=lrn_rate, loss_func= func,plr=h,beta=0.9, eps=eps,term_eps=eps_t),\n",
        "    \"AngularGradCos\" :  AngularGradCos(params = var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"AngularGradTan\" : AngularGradTan(params = var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"Adam\" : Adam(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"RMSprop\" : RMSprop(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"AdaBelief\" : AdaBelief(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"diffGrad\" : diffGrad(params=var, lrn_rate=lrn_rate, beta1=beta1, beta2=beta2, eps=eps),\n",
        "    \"SGDM\" : SGDM(params=var, lrn_rate=lrn_rate, beta1=beta1, eps=eps),\n",
        "    \"Ours\" :  Ours(params=var, steps=1, h=lrn_rate, loss_func= func,plr=h,beta=0.9, eps=eps,term_eps=eps_t),\n",
        "    \n",
        "    \n",
        "\n",
        "  }\n",
        "  method_name = list(optimizers.keys())[i]\n",
        "  optimizer = optimizers[method_name]\n",
        "  print(method_name)\n",
        "  points = []\n",
        "  x_coord, y_coord = optimizer.params\n",
        "  z_coord = func_graph(optimizer.params.detach())\n",
        "  x_coord = x_coord.item()\n",
        "  y_coord = y_coord.item()\n",
        "  z_coord = z_coord.item()\n",
        "  points.append([x_coord, y_coord, z_coord])\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    #print(epoch)\n",
        "    loss_value = func()\n",
        "    optimizer.step(loss_value)\n",
        "    x_coord, y_coord = optimizer.params\n",
        "    z_coord = func_graph(optimizer.params.detach())\n",
        "    x_coord = x_coord.item()\n",
        "    y_coord = y_coord.item()\n",
        "    z_coord = z_coord.item()\n",
        "    points.append([x_coord, y_coord, z_coord])\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "  points = np.array(points)\n",
        "  #points = np.array([[-2,2,func_1([-2,2])], [-2,0, func_1([-2,0])],])# [-2,1, func_1([-2,1])], [0,0, func_1([0,0])]])\n",
        "  fig = plt.figure()\n",
        "  axes = fig.gca(projection ='3d', computed_zorder=False)\n",
        "  #print(\"OPTIMAL \", optimizer.params)\n",
        "\n",
        "  x = np.arange(-4,4,0.1)\n",
        "  y = np.arange(-4,4,0.1)\n",
        "  axis_x,axis_y = np.meshgrid(x,y)\n",
        "\n",
        "\n",
        "\n",
        "  z = func_graph([axis_x, axis_y])\n",
        "  #print(axis_x)\n",
        "\n",
        "  surf = axes.plot_surface(axis_x, axis_y, z, alpha=0.6, zorder=0,)\n",
        "  surf._facecolors2d = surf._facecolor3d\n",
        "  surf._edgecolors2d = surf._edgecolor3d\n",
        "  #axes.plot3D(points[:,0], points[:,1], points[:,2],'red')\n",
        "\n",
        "  for i,v in enumerate(points[:len(points)-1]):\n",
        "      #ax.plot([mean_x,v[0]], [mean_y,v[1]], [mean_z,v[2]], color='red', alpha=0.8, lw=3)\n",
        "      #I will replace this line with:\n",
        "      a = Arrow3D([points[i,0], points[i+1, 0]], [points[i,1], points[i+1, 1]], \n",
        "                  [points[i,2], points[i+1, 2]], mutation_scale=10, \n",
        "                  lw=2, arrowstyle=\"-|>\", color=\"red\")\n",
        "      a.set_zorder(1)\n",
        "      axes.add_artist(a)\n",
        "\n",
        "  axes.set_xlabel(\"x\")\n",
        "  axes.set_ylabel(\"y\")\n",
        "  axes.set_zlabel(\"function value\", rotation=45)\n",
        "  axes.set_title(method_name,fontsize=20)\n",
        "\n",
        "  for point in points[1:len(points)-1]:\n",
        "    axes.scatter3D(points[:,0], points[:,1], points[:,2], c='red', zorder = 3)\n",
        "  axes.scatter3D(start_point[0],start_point[1], func_graph(torch.tensor(start_point)), s = 140, c='orange' ,marker='*' , zorder = 3,label='start')\n",
        "  axes.scatter3D(points[-1,0], points[-1,1], points[-1,2], s=140, c='black' ,marker='*' , zorder = 3, label= 'end')\n",
        "  axes.legend(loc=\"upper right\", bbox_to_anchor=(1.15,0.8))\n",
        "  axes.view_init(40, 60)\n",
        "  #axes.view_init(40, 60)\n",
        "  fig.savefig(method_name+\".png\", dpi=100, format='png', bbox_inches='tight')\n",
        "  fig.clf()\n",
        "  plt.clf()"
      ],
      "metadata": {
        "id": "cmiIjvHfLLaj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "c21b2df0-8112-4105-cdf3-788009cc7ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ours\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: MatplotlibDeprecationWarning: \n",
            "The M attribute was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use self.axes.M instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AngularGradCos\n",
            "AngularGradTan\n",
            "Adam\n",
            "RMSprop\n",
            "AdaBelief\n",
            "diffGrad\n",
            "SGDM\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRVk3QfrskEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}